"""
ä¸»ç¨‹åºå…¥å£ - åˆå§‹åŒ–å’Œå¯åŠ¨åº”ç”¨ï¼ˆPyTorch + CUDA GPUï¼‰
"""

import os
import tkinter as tk
from tkinter import messagebox
import sys
from pathlib import Path

# ç¦ç”¨Hugging Face symlinkè­¦å‘Šï¼ˆWindowsä¸æ”¯æŒï¼‰
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

from model_manager import ModelManager
from inference_engine import InferenceEngine
from ui import QwenGUI
from config import MODEL_NAME, DEVICE


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    
    print("=" * 60)
    print("[å¯åŠ¨] Qwen2.5-7B æœ¬åœ°æ¨ç†ç³»ç»Ÿ (PyTorch + CUDA GPU)")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    model_manager = ModelManager()
    
    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    device_info = model_manager.get_device_info()
    print(f"\n[ç³»ç»Ÿä¿¡æ¯]")
    print(f"  - æ¨ç†è®¾å¤‡: {device_info['device'].upper()}")
    print(f"  - CUDAå¯ç”¨: {device_info['cuda_available']}")
    print(f"  - æ¨¡å‹ç²¾åº¦: {device_info['model_dtype']}")
    
    if device_info['cuda_available']:
        print(f"  - GPUå‹å·: {device_info.get('gpu_name', 'N/A')}")
        print(f"  - GPUæ˜¾å­˜: {device_info.get('gpu_memory_total', 'N/A')}")
        print(f"  - SMèƒ½åŠ›: {device_info.get('sm_capability', 'N/A')}")
    
    print(f"  - PyTorchç‰ˆæœ¬: {device_info['torch_version']}")
    print(f"  - æ¨¡å‹: {MODEL_NAME}\n")
    
    # åˆ›å»ºTkinteræ ¹çª—å£
    root = tk.Tk()
    root.withdraw()  # å…ˆéšè—çª—å£
    
    try:
        # æ˜¾ç¤ºåŠ è½½æç¤º
        messagebox.showinfo(
            "åŠ è½½æ¨¡å‹",
            "æ­£åœ¨åŠ è½½Qwen2.5-7B...\n\n"
            "é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹\n"
            "è¿™å¯èƒ½éœ€è¦10-20åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…..."
        )
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        if not model_manager.download_model():
            messagebox.showerror("é”™è¯¯", "æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’Œå†…å­˜")
            root.destroy()
            return
        
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨ç†å¼•æ“...")
        model = model_manager.get_model()
        tokenizer = model_manager.get_tokenizer()
        inference_engine = InferenceEngine(model, tokenizer)
        
        print("âœ… æ¨¡å‹å’Œæ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ\n")
        
        # æ˜¾ç¤ºçª—å£å¹¶åˆ›å»ºGUI
        root.deiconify()
        gui = QwenGUI(root, inference_engine)
        
        # æ¬¢è¿ä¿¡æ¯
        gui.display_message(
            "ç³»ç»Ÿ",
            "æ¬¢è¿ä½¿ç”¨Qwen2.5-7Bæœ¬åœ°æ¨ç†åŠ©æ‰‹ï¼\n"
            "åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚\n"
            "æŒ‰ Ctrl+Enter å‘é€æ¶ˆæ¯ã€‚",
            "system"
        )
        
        print("=" * 60)
        print("âœ… åº”ç”¨å¯åŠ¨å®Œæˆï¼Œçª—å£å·²æ‰“å¼€")
        print("=" * 60)
        
        # å¯åŠ¨GUIä¸»å¾ªç¯
        root.mainloop()
    
    except Exception as e:
        messagebox.showerror("é”™è¯¯", f"åº”ç”¨å¯åŠ¨å¤±è´¥:\n{str(e)}")
        root.destroy()
        sys.exit(1)


if __name__ == "__main__":
    main()
