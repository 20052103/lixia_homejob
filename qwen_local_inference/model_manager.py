"""
æ¨¡å‹ç®¡ç†å™¨ - åŠ è½½å’Œç®¡ç†Qwen2.5-7Bæ¨¡å‹
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from config import MODEL_NAME, MODEL_CACHE_DIR, DEVICE, DTYPE, USE_FLASH_ATTENTION


class ModelManager:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self._ensure_cache_dir()
    
    def _ensure_cache_dir(self):
        """ç¡®ä¿æ¨¡å‹ç¼“å­˜ç›®å½•å­˜åœ¨"""
        os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
    
    def download_model(self):
        """ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼ˆå·²é¢„ä¸‹è½½ï¼‰"""
        try:
            # é¦–å…ˆå°è¯•ä»æœ¬åœ°ç›®å½•åŠ è½½ï¼ˆæ— éœ€ç½‘ç»œï¼‰
            local_model_path = os.path.join(MODEL_CACHE_DIR, "models--Qwen--Qwen2.5-7B-Instruct", "snapshots")
            
            if os.path.exists(local_model_path):
                # è·å–ç¬¬ä¸€ä¸ªsnapshotç›®å½•
                snapshots = os.listdir(local_model_path)
                if snapshots:
                    snapshot_path = os.path.join(local_model_path, snapshots[0])
                    print(f"ğŸ“‚ ä»æœ¬åœ°åŠ è½½æ¨¡å‹: {snapshot_path}")
                    
                    # åŠ è½½åˆ†è¯å™¨
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        snapshot_path,
                        trust_remote_code=True,
                        local_files_only=True
                    )
                    
                    # åŠ è½½æ¨¡å‹
                    self.model = AutoModelForCausalLM.from_pretrained(
                        snapshot_path,
                        device_map="auto",
                        dtype=torch.float16 if DTYPE == "auto" else DTYPE,
                        trust_remote_code=True,
                        local_files_only=True,
                        attn_implementation="flash_attention_2" if USE_FLASH_ATTENTION else None
                    )
                    
                    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
                    return True
            
            # å¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼Œåˆ™ä»HuggingFaceä¸‹è½½
            print(f"ğŸ“¥ ä»HuggingFaceåŠ è½½ {MODEL_NAME}...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                MODEL_NAME,
                cache_dir=MODEL_CACHE_DIR,
                trust_remote_code=True
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                cache_dir=MODEL_CACHE_DIR,
                device_map="auto",
                dtype=torch.float16 if DTYPE == "auto" else DTYPE,
                trust_remote_code=True,
                attn_implementation="flash_attention_2" if USE_FLASH_ATTENTION else None
            )
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_model(self):
        return self.model
    
    def get_tokenizer(self):
        return self.tokenizer
    
    def get_device_info(self):
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        info = {
            "device": DEVICE,
            "cuda_available": torch.cuda.is_available(),
            "torch_version": torch.__version__,
            "model_dtype": str(self.model.dtype) if self.model else "N/A"
        }
        
        if torch.cuda.is_available():
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["gpu_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB"
            info["sm_capability"] = f"SM_{torch.cuda.get_device_capability(0)[0]}{torch.cuda.get_device_capability(0)[1]}"
        
        return info
